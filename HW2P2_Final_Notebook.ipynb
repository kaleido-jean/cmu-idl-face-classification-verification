{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aD8oEOiY3kG8"
      },
      "source": [
        "## **PSC Users**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g5XKN5jZ3kG8"
      },
      "source": [
        "### 1️⃣ **Step 1 Setting Up Your Environment on Bridges2**\n",
        "\n",
        "❗️⚠️ For this homework, we are **providing shared Datasets and a shared Conda environment** for the entire class.\n",
        "\n",
        "❗️⚠️ So for PSC users, **do not download the data yourself** and **do not need to manually install the packages**!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84eOp-UY3kG8"
      },
      "source": [
        "Follow these steps to set up the environment and start a Jupyter notebook on Bridges2:\n",
        "\n",
        "To run your notebook more efficiently on PSC, we need to use a **Jupyter Server** hosted on a compute node.\n",
        "\n",
        "You can use your prefered way of connecting to the Jupyter Server.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P5gfdJzx3kG8"
      },
      "source": [
        "**The recommended way of connecting is:**\n",
        "\n",
        "#### **Connect in VSCode**\n",
        "SSH into Bridges2 and navigate to your **Jet directory** (`Jet/home/<your_username>`). Upload your notebook there, and then connect to the Jupyter Server from that directory."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EutBOttx3kG8"
      },
      "source": [
        "#### **1. SSH into Bridges2**\n",
        "1）Open VS Code and click on the `Extensions` icon in the left sidebar. Make sure the \"**Remote - SSH**\" extension is installed.\n",
        "\n",
        "2）Open the command palette (**Shift+Command+P** on Mac, **Ctrl+Shift+P** on Windows). A search box will appear at the top center. Choose `\"Remote-SSH: Add New SSH Host\"`, then enter:\n",
        "\n",
        "```bash\n",
        "ssh <your_username>@bridges2.psc.edu #change <your_username> to your username\n",
        "```\n",
        "\n",
        "Next, choose `\"/Users/<your_username>/.ssh/config\"` as the config file. A dialog will appear in the bottom right saying \"Host Added\". Click `\"Connect\"`, and then enter your password.\n",
        "\n",
        "(Note: After adding the host once, you can later use `\"Remote-SSH: Connect to Host\"` and select \"bridges2.psc.edu\" from the list.)\n",
        "\n",
        "3）Once connected, click `\"Explorer\"` in the left sidebar > \"Open Folder\", and navigate to your home directory under the project grant:\n",
        "```bash\n",
        "/jet/home/<your_username>  #change <your_username> to your username\n",
        "```\n",
        "\n",
        "4）You can now drag your notebook files directly into the right-hand pane (your remote home directory), or upload them using `scp` into your folder."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wzoiexf53kG8"
      },
      "source": [
        "> ❗️⚠️ The following steps should be executed in the **VSCode integrated terminal**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62j2RAEA3kG8"
      },
      "source": [
        "#### **2. Navigate to Your Directory**\n",
        "Make sure to use this `/jet/home/<your_username>` as your working directory, since all subsequent operations (up to submission) are based on this path.\n",
        "```bash\n",
        "cd /jet/home/<your_username>  #change <your_username> to your username\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HavF1IlO3kG9"
      },
      "source": [
        "#### **3. Request a Compute Node**\n",
        "```bash\n",
        "interact -p GPU-shared --gres=gpu:v100-32:1 -t 8:00:00 -A cis250019p\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5l6iDrZ3kG9"
      },
      "source": [
        "#### **4. Load the Anaconda Module**\n",
        "```bash\n",
        "module load anaconda3\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rsnoo8SU3kG9"
      },
      "source": [
        "#### **5. Activate the provided HW2 Environment**\n",
        "```bash\n",
        "conda deactivate # First, deactivate any existing Conda environment\n",
        "conda activate /ocean/projects/cis250019p/mzhang23/TA/HW2P2/envs/hw2p2_env && export PYTHONNOUSERSITE=1\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "da-B39u83kG9"
      },
      "source": [
        "#### **6. Start Jupyter Notebook**\n",
        "Launch Jupyter Notebook:\n",
        "```bash\n",
        "jupyter notebook --no-browser --ip=0.0.0.0\n",
        "```\n",
        "\n",
        "Go to **Kernel** → **Select Another Kernel** → **Existing Jupyter Server**\n",
        "   Enter the URL of the Jupyter Server:```http://{hostname}:{port}/tree?token={token}```\n",
        "   \n",
        "   *(Usually, this URL appears in the terminal output after you run `jupyter notebook --no-browser --ip=0.0.0.0`, in a line like:  “Jupyter Server is running at: http://...”)*\n",
        "\n",
        "   - eg: `http://v011.ib.bridges2.psc.edu:8888/tree?token=e4b302434e68990f28bc2b4ae8d216eb87eecb7090526249`\n",
        "\n",
        "> **Note**: Replace `{hostname}`, `{port}` and `{token}` with your actual values from the Jupyter output."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M3VWeHyS3kG9"
      },
      "source": [
        "#### **7. Navigate to Your Jet Directory**\n",
        "\n",
        "After launching the Jupyter notebook, you can run the cells directly inside the notebook — no need to use the terminal for the remaining steps.\n",
        "\n",
        "First, navigate to your **Jet directory** (`/jet/home/<your_username>`).\n",
        "\n",
        "️❗️⚠ Please make sure to use your **Jet directory**, not the **Ocean path** — **all HW setup and outputs below are based on this directory**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TOnI9xHM3kHC"
      },
      "outputs": [],
      "source": [
        "#Make sure you are in your directory\n",
        "!pwd #should be /jet/home/<your_username>, if not, uncomment the following line and replace with your actual username:\n",
        "%cd /jet/home/<your_username> #TODO: replace the \"<your_username>\" to yours"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TrdNB7VI3kHD"
      },
      "source": [
        "### 2️⃣ **Step 2: Set up Kaggle API Authentication**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iXxci9RA3kHD"
      },
      "outputs": [],
      "source": [
        "# TODO: Use the same Kaggle code from HW1P2\n",
        "!mkdir /jet/home/<your_username>/.kaggle #TODO: replace the \"<your_username>\" to yours\n",
        "\n",
        "with open(\"/jet/home/<your_username>/.kaggle/kaggle.json\", \"w+\") as f: #TODO: replace the \"<your_username>\" to yours\n",
        "    f.write('{\"username\":\"TODO\",\"key\":\"TODO\"}')\n",
        "    # TODO: Put your kaggle username & key here\n",
        "\n",
        "!chmod 600 /jet/home/<your_username>/.kaggle/kaggle.json #TODO: replace the \"<your_username>\" to yours"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yHgQ2mR_3kHD"
      },
      "source": [
        "### 3️⃣ **Step 3: Get Data**\n",
        "\n",
        "❗️⚠️ The data used in this assignment is **already stored in a shared, read-only folder, so you do not need to manually download anything**.\n",
        "\n",
        "Instead, just make sure to replace the dataset path in your notebook code with the correct path from the shared directory.\n",
        "\n",
        "You can run the following block to explore the shared directory structure:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3YNxFJYL3kHD"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "data_path = \"/ocean/projects/cis250019p/mzhang23/TA/HW2P2/hw2p2_data/hw2p2_puru_aligned\" #Shared data path, do not need to change the username to yours\n",
        "print(\"Files in shared hw2p2 dataset:\", os.listdir(data_path))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sBE_Tqy33kHD"
      },
      "outputs": [],
      "source": [
        "!apt-get install tree\n",
        "!tree -L 2 /ocean/projects/cis250019p/mzhang23/TA/HW2P2/hw2p2_data/hw2p2_puru_aligned"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q1XE_p9OsQGp"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GKZw2YsvsPZV"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "# from torchsummary import summary\n",
        "from torchinfo import summary\n",
        "import torchvision\n",
        "from torchvision.utils import make_grid\n",
        "from torchvision import transforms\n",
        "import torchvision.transforms.v2 as T\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "import os\n",
        "import gc\n",
        "# from tqdm import tqdm\n",
        "from tqdm.auto import tqdm\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn import metrics as mt\n",
        "from scipy.optimize import brentq\n",
        "from scipy.interpolate import interp1d\n",
        "import glob\n",
        "import wandb\n",
        "import matplotlib.pyplot as plt\n",
        "from pytorch_metric_learning import samplers\n",
        "import csv\n",
        "import pdb\n",
        "\n",
        "# from timm.models.layers import trunc_normal_, DropPath\n",
        "\n",
        "if torch.backends.mps.is_available():\n",
        "    device = torch.device(\"mps\")  \n",
        "elif torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\") \n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "print(\"Device:\", device) \n",
        "\n",
        "DEVICE = device.type\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9OgkfYwP7HVt"
      },
      "source": [
        "# Config"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YnmbQ6VntU5D"
      },
      "source": [
        "### Notes:\n",
        "\n",
        "- You will need to set the root path to your `hw2p2_data` folder in `data: root:`. This will depend on your setup. For eg. if you are following out setup instruction:\n",
        "  - `Colab:`: `\"/content/data/hw2p2_puru_aligned\"`\n",
        "  - `Kaggle:`: `\"/kaggle/input/11785-hw-2-p-2-face-verification-fall-2025/hw2p2_puru_aligned\"`\n",
        "  - `PSC`: `\"/ocean/projects/cis250019p/mzhang23/TA/HW2P2/hw2p2_data/hw2p2_puru_aligned\"`\n",
        "\n",
        "Kindly modify your configurations to suit your ablations and be keen to include your name."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CMXkHmFc7G9m"
      },
      "outputs": [],
      "source": [
        "config = {\n",
        "    'batch_size': 256, # Increase this if your GPU can handle it\n",
        "    'lr': 0.004,\n",
        "    'epochs': 50, # 20 epochs is recommended ONLY for the early submission - you will have to train for much longer typically.\n",
        "    'num_classes': int(8631*1), #Dataset contains 8631 classes for classification, reduce this number if you want to train on a subset, but only for train dataset and not on val dataset\n",
        "    'cls_data_dir': \"/ocean/projects/cis250019p/mzhang23/TA/HW2P2/hw2p2_data/hw2p2_puru_aligned/cls_data\", #TODO: Provide path of classification directory\n",
        "    'ver_data_dir': \"/ocean/projects/cis250019p/mzhang23/TA/HW2P2/hw2p2_data/hw2p2_puru_aligned/ver_data\", #TODO: Provide path of verification directory\n",
        "    'val_pairs_file': \"/ocean/projects/cis250019p/mzhang23/TA/HW2P2/hw2p2_data/hw2p2_puru_aligned/val_pairs.txt\", #TODO: Provide path of text file containing val pairs for verification\n",
        "    'test_pairs_file': \"/ocean/projects/cis250019p/mzhang23/TA/HW2P2/hw2p2_data/hw2p2_puru_aligned/test_pairs.txt\", #TODO: Provide path of text file containing test pairs for verification\n",
        "    'checkpoint_dir': \"<YOUR PATH>/checkpoints\", #TODO: Checkpoint directory\n",
        "    'augument': True\n",
        "    # Include other parameters as needed.\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EEAW65sB8Wlp"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x-jrgnbQyR2s"
      },
      "outputs": [],
      "source": [
        "def create_transforms(image_size: int = 112, augment: bool = True) -> T.Compose:\n",
        "    \"\"\"Create transform pipeline for face recognition.\"\"\"\n",
        "\n",
        "    # Step 1: Basic transformations\n",
        "    transform_list = [\n",
        "        # Resize the image to the desired size (image_size x image_size)\n",
        "        T.Resize((image_size, image_size)),\n",
        "\n",
        "        # Convert PIL Image to tensor\n",
        "        T.ToTensor(),\n",
        "\n",
        "        # Convert image to float32 and scale the pixel values to [0, 1]\n",
        "        T.ToDtype(torch.float32, scale=True),\n",
        "    ]\n",
        "\n",
        "    # Step 2: Data augmentation (optional, based on `augment` argument)\n",
        "    if augment:  # This block will be executed if `augment=True`\n",
        "        # TODO: Add transformations for data augmentation (e.g., random horizontal flip, rotation, etc.)\n",
        "        # HINT: What transforms help faces look more varied?\n",
        "        # Think: Does a horizontally flipped face still look like the same person?\n",
        "        # What about small rotations or color changes?\n",
        "        # Example:\n",
        "        transform_list.extend([\n",
        "            # Your transforms here\n",
        "            T.RandomHorizontalFlip(p=0.5),\n",
        "            T.RandomRotation(degrees=(-10,10)),\n",
        "            T.ColorJitter(brightness=0.15, contrast=0.15, saturation=0.1, hue=0.1)\n",
        "        ])\n",
        "\n",
        "\n",
        "    # Step 3: Standard normalization for image recognition tasks\n",
        "    # The Normalize transformation requires mean and std values for each channel (R, G, B).\n",
        "    # Here, we are normalizing the pixel values to have a mean of 0.5 and std of 0.5 for each channel.\n",
        "    transform_list.extend([\n",
        "        T.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])  # Standard mean and std for face recognition tasks\n",
        "    ])\n",
        "\n",
        "    # Return the composed transformation pipeline\n",
        "    return T.Compose(transform_list)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZzxNNzGe8ccB"
      },
      "source": [
        "## Classification Datasets and Dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p6Nn57B-7F-i"
      },
      "outputs": [],
      "source": [
        "class ImageDataset(torch.utils.data.Dataset):\n",
        "    \"\"\"Custom dataset for loading image-label pairs.\"\"\"\n",
        "    def __init__(self, root, transform, num_classes=config['num_classes'], partition='train'):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            root (str): Path to the directory containing the images folder.\n",
        "            transform (callable): Transform to be applied to the images.\n",
        "            num_classes (int, optional): Number of classes to keep. If None, keep all classes.\n",
        "        \"\"\"\n",
        "        self.root = root + '/' + partition\n",
        "        self.labels_file = os.path.join(self.root, \"labels.txt\")\n",
        "        self.transform = transform\n",
        "        self.image_paths = []\n",
        "        self.labels = []\n",
        "        self.classes = set()\n",
        "\n",
        "        # Read image-label pairs from the file\n",
        "        with open(self.labels_file, 'r') as f:\n",
        "            lines = f.readlines()\n",
        "\n",
        "        lines = sorted(lines, key=lambda x: int(x.strip().split(' ')[-1]))\n",
        "\n",
        "        # Get all unique labels first\n",
        "        all_labels = sorted(set(int(line.strip().split(' ')[1]) for line in lines))\n",
        "\n",
        "        # Select subset of classes if specified\n",
        "        if num_classes is not None:\n",
        "            selected_classes = set(all_labels[:num_classes])\n",
        "        else:\n",
        "            selected_classes = set(all_labels)\n",
        "\n",
        "        # Store image paths and labels with a progress bar\n",
        "        for line in tqdm(lines, desc=\"Loading dataset\"):\n",
        "            img_path, label = line.strip().split(' ')\n",
        "            label = int(label)\n",
        "\n",
        "            # Only add if label is in selected classes\n",
        "            if label in selected_classes:\n",
        "                self.image_paths.append(os.path.join(self.root, 'images', img_path))\n",
        "                self.labels.append(label)\n",
        "                self.classes.add(label)\n",
        "\n",
        "        assert len(self.image_paths) == len(self.labels), \"Images and labels mismatch!\"\n",
        "\n",
        "        # Convert classes to a sorted list\n",
        "        self.classes = sorted(self.classes)\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Returns the total number of samples.\"\"\"\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            idx (int): Index of the sample to retrieve.\n",
        "\n",
        "        Returns:\n",
        "            tuple: (transformed image, label)\n",
        "        \"\"\"\n",
        "        # Load and transform image on-the-fly\n",
        "        image = Image.open(self.image_paths[idx]).convert('RGB')\n",
        "        image = self.transform(image)\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        return image, label\n",
        "\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f3d1tsYE0pb-"
      },
      "outputs": [],
      "source": [
        "# train transforms\n",
        "train_transforms = create_transforms(augment=config['augument'])\n",
        "\n",
        "# val transforms\n",
        "val_transforms   = create_transforms(augment=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GOJOrQLX02Gh"
      },
      "outputs": [],
      "source": [
        "# Datasets\n",
        "cls_train_dataset = ImageDataset(root=config['cls_data_dir'], num_classes=config['num_classes'], transform=train_transforms, partition='train')#TODO\n",
        "print(len(cls_train_dataset.classes))\n",
        "# HINT: What dataset class do you use? What folder has your training data?\n",
        "# What transforms should training data use?\n",
        "\n",
        "cls_val_dataset   = ImageDataset(root=config['cls_data_dir'], num_classes=config['num_classes'], transform=val_transforms, partition='dev')#TODO\n",
        "# HINT: Same dataset class, but what folder for validation?\n",
        "# Should validation use augmentation transforms?\n",
        "\n",
        "cls_test_dataset  = ImageDataset(root=config['cls_data_dir'], num_classes=config['num_classes'], transform=val_transforms, partition='test')#TODO\n",
        "# HINT: What's the pattern here? What folder contains test data?\n",
        "\n",
        "# assert cls_train_dataset.classes == cls_val_dataset.classes == cls_test_dataset.classes, \"Class mismatch!\"\n",
        "\n",
        "\n",
        "# Dataloaders\n",
        "cls_train_loader = DataLoader(cls_train_dataset, batch_size=config['batch_size'], shuffle=True,  num_workers=4, pin_memory=True)\n",
        "cls_val_loader   = DataLoader(cls_val_dataset,   batch_size=config['batch_size'], shuffle=False, num_workers=4, pin_memory=True)\n",
        "cls_test_loader  = DataLoader(cls_test_dataset,  batch_size=config['batch_size'], shuffle=False, num_workers=4, pin_memory=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### MixUp \\& CutMix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cutmix = T.CutMix(num_classes=config['num_classes'])\n",
        "mixup = T.MixUp(num_classes=config['num_classes'])\n",
        "cutmix_or_mixup = T.RandomChoice([cutmix, mixup],p=[1,0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mPSk8DyK8htk"
      },
      "source": [
        "## Verification Dataset and Datatloaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KBleUieO8lwG"
      },
      "outputs": [],
      "source": [
        "\n",
        "class ImagePairDataset(torch.utils.data.Dataset):\n",
        "    \"\"\"Custom dataset for loading and transforming image pairs.\"\"\"\n",
        "    def __init__(self, root, pairs_file, transform):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            root (str): Path to the directory containing the images.\n",
        "            pairs_file (str): Path to the file containing image pairs and match labels.\n",
        "            transform (callable): Transform to be applied to the images.\n",
        "        \"\"\"\n",
        "        self.root      = root\n",
        "        self.transform = transform\n",
        "\n",
        "        self.matches     = []\n",
        "        self.image1_list = []\n",
        "        self.image2_list = []\n",
        "\n",
        "        # Read and load image pairs and match labels\n",
        "        with open(pairs_file, 'r') as f:\n",
        "            lines = f.readlines()\n",
        "\n",
        "        for line in tqdm(lines, desc=\"Loading image pairs\"):\n",
        "            img_path1, img_path2, match = line.strip().split(' ')\n",
        "            img1 = Image.open(os.path.join(self.root, img_path1)).convert('RGB')\n",
        "            img2 = Image.open(os.path.join(self.root, img_path2)).convert('RGB')\n",
        "\n",
        "            self.image1_list.append(img1)\n",
        "            self.image2_list.append(img2)\n",
        "            self.matches.append(int(match))  # Convert match to integer\n",
        "\n",
        "        assert len(self.image1_list) == len(self.image2_list) == len(self.matches), \"Image pair mismatch\"\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Returns the total number of samples.\"\"\"\n",
        "        return len(self.image1_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            idx (int): Index of the sample to retrieve.\n",
        "\n",
        "        Returns:\n",
        "            tuple: (transformed image1, transformed image2, match label)\n",
        "        \"\"\"\n",
        "        img1 = self.image1_list[idx]\n",
        "        img2 = self.image2_list[idx]\n",
        "        match = self.matches[idx]\n",
        "        return self.transform(img1), self.transform(img2), match\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BxXF96_Ys8os"
      },
      "outputs": [],
      "source": [
        "\n",
        "class TestImagePairDataset(torch.utils.data.Dataset):\n",
        "    \"\"\"Custom dataset for loading and transforming image pairs.\"\"\"\n",
        "    def __init__(self, root, pairs_file, transform):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            root (str): Path to the directory containing the images.\n",
        "            pairs_file (str): Path to the file containing image pairs and match labels.\n",
        "            transform (callable): Transform to be applied to the images.\n",
        "        \"\"\"\n",
        "        self.root      = root\n",
        "        self.transform = transform\n",
        "\n",
        "        self.image1_list = []\n",
        "        self.image2_list = []\n",
        "\n",
        "        # Read and load image pairs and match labels\n",
        "        with open(pairs_file, 'r') as f:\n",
        "            lines = f.readlines()\n",
        "\n",
        "        for line in tqdm(lines, desc=\"Loading image pairs\"):\n",
        "            img_path1, img_path2 = line.strip().split(' ')\n",
        "            img1 = Image.open(os.path.join(self.root, img_path1)).convert('RGB')\n",
        "            img2 = Image.open(os.path.join(self.root, img_path2)).convert('RGB')\n",
        "\n",
        "            self.image1_list.append(img1)\n",
        "            self.image2_list.append(img2)\n",
        "\n",
        "        assert len(self.image1_list) == len(self.image2_list), \"Image pair mismatch\"\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Returns the total number of samples.\"\"\"\n",
        "        return len(self.image1_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            idx (int): Index of the sample to retrieve.\n",
        "\n",
        "        Returns:\n",
        "            tuple: (transformed image1, transformed image2, match label)\n",
        "        \"\"\"\n",
        "        img1 = self.image1_list[idx]\n",
        "        img2 = self.image2_list[idx]\n",
        "        return self.transform(img1), self.transform(img2)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ALvOc6L2r3d"
      },
      "outputs": [],
      "source": [
        "# Datasets\n",
        "ver_val_dataset  = ImagePairDataset(root=config['ver_data_dir'], pairs_file=config['val_pairs_file'], transform=val_transforms)#TODO\n",
        "# HINT: What dataset class handles image pairs? What file lists the validation pairs?\n",
        "\n",
        "ver_test_dataset = TestImagePairDataset(root=config['ver_data_dir'], pairs_file=config['test_pairs_file'], transform=val_transforms)#TODO\n",
        "# HINT: Same class, but what file has test pairs? Does it include labels?\n",
        "\n",
        "# Dataloader\n",
        "ver_val_loader   = DataLoader(ver_val_dataset,  batch_size=config['batch_size'], shuffle=False, num_workers=4, pin_memory=True)\n",
        "ver_test_loader  = DataLoader(ver_test_dataset, batch_size=config['batch_size'], shuffle=False, num_workers=4, pin_memory=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2j24TXNo9P97"
      },
      "source": [
        "## Create Dataloaders for Image Recognition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "436KzM6u-3A2"
      },
      "source": [
        "# EDA and Viz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AhnoHopx-0RB"
      },
      "outputs": [],
      "source": [
        "# Double-check your dataset/dataloaders work as expected\n",
        "\n",
        "print(\"Number of classes    : \", len(cls_train_dataset.classes))\n",
        "print(\"No. of train images  : \", cls_train_dataset.__len__())\n",
        "print(\"Shape of image       : \", cls_train_dataset[0][0].shape)\n",
        "print(\"Batch size           : \", config['batch_size'])\n",
        "print(\"Train batches        : \", cls_train_loader.__len__())\n",
        "print(\"Val batches          : \", cls_val_loader.__len__())\n",
        "\n",
        "# Feel free to print more things if needed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "niSJ49lzpHgW"
      },
      "source": [
        "### Classification Dataset Viz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fK3t65VU5Qlz"
      },
      "outputs": [],
      "source": [
        "def show_cls_dataset_samples(train_loader, val_loader, test_loader, samples_per_set=8, figsize=(10, 6)):\n",
        "    \"\"\"\n",
        "    Display samples from train, validation, and test datasets side by side\n",
        "\n",
        "    Args:\n",
        "        train_loader: Training data loader\n",
        "        val_loader: Validation data loader\n",
        "        test_loader: Test data loader\n",
        "        samples_per_set: Number of samples to show from each dataset\n",
        "        figsize: Figure size (width, height)\n",
        "    \"\"\"\n",
        "    def denormalize(x):\n",
        "        \"\"\"Denormalize images from [-1, 1] to [0, 1]\"\"\"\n",
        "        return x * 0.5 + 0.5\n",
        "\n",
        "    def get_samples(loader, n):\n",
        "        \"\"\"Get n samples from a dataloader\"\"\"\n",
        "        batch = next(iter(loader))\n",
        "        return batch[0][:n], batch[1][:n]\n",
        "\n",
        "    # Get samples from each dataset\n",
        "    train_imgs, train_labels = get_samples(train_loader, samples_per_set)\n",
        "    # pdb.set_trace()\n",
        "    # train_imgs, train_labels = cutmix_or_mixup(train_imgs, train_labels)\n",
        "\n",
        "    val_imgs, val_labels = get_samples(val_loader, samples_per_set)\n",
        "    test_imgs, test_labels = get_samples(test_loader, samples_per_set)\n",
        "\n",
        "    # Create figure\n",
        "    fig, axes = plt.subplots(3, 1, figsize=figsize)\n",
        "\n",
        "    # Plot each dataset\n",
        "    for idx, (imgs, labels, title) in enumerate([\n",
        "        (train_imgs, train_labels, 'Training Samples'),\n",
        "        (val_imgs, val_labels, 'Validation Samples'),\n",
        "        (test_imgs, test_labels, 'Test Samples')\n",
        "    ]):\n",
        "\n",
        "        # Create grid of images\n",
        "        grid = make_grid(denormalize(imgs), nrow=8, padding=2)\n",
        "\n",
        "        # Display grid\n",
        "        axes[idx].imshow(grid.permute(1, 2, 0).cpu())\n",
        "        axes[idx].axis('off')\n",
        "        axes[idx].set_title(title, fontsize=10)\n",
        "\n",
        "        # Add class labels below images (with smaller font)\n",
        "        grid_width = grid.shape[2]\n",
        "        imgs_per_row = min(8, samples_per_set)\n",
        "        img_width = grid_width // imgs_per_row\n",
        "\n",
        "        for i, label in enumerate(labels):\n",
        "            col = i % imgs_per_row  # Calculate column position\n",
        "            if label<len(train_loader.dataset.classes):\n",
        "              class_name = train_loader.dataset.classes[label]\n",
        "            else:\n",
        "              class_name = f\"Class {label} (Unknown)\"\n",
        "            axes[idx].text(col * img_width + img_width/2,\n",
        "                         grid.shape[1] + 5,\n",
        "                         class_name,\n",
        "                         ha='center',\n",
        "                         va='top',\n",
        "                         fontsize=6,\n",
        "                         rotation=45)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "show_cls_dataset_samples(cls_train_loader, cls_val_loader, cls_test_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KZPVGXOu59Wh"
      },
      "source": [
        "### Ver Dataset Viz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6pBN7Z9K5iAM"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from torchvision.utils import make_grid\n",
        "\n",
        "def show_ver_dataset_samples(val_loader, samples_per_set=4, figsize=(12, 8)):\n",
        "    \"\"\"\n",
        "    Display verification pairs from the validation dataset\n",
        "\n",
        "    Args:\n",
        "        val_loader: Validation data loader\n",
        "        samples_per_set: Number of pairs to show from the dataset\n",
        "        figsize: Figure size (width, height)\n",
        "    \"\"\"\n",
        "    def denormalize(x):\n",
        "        \"\"\"Denormalize images from [-1, 1] to [0, 1]\"\"\"\n",
        "        return x * 0.5 + 0.5\n",
        "\n",
        "    def get_samples(loader, n):\n",
        "        \"\"\"Get n samples from a dataloader\"\"\"\n",
        "        batch = next(iter(loader))\n",
        "        return batch[0][:n], batch[1][:n], batch[2][:n]\n",
        "\n",
        "    # Get samples from the validation dataset\n",
        "    val_imgs1, val_imgs2, val_labels = get_samples(val_loader, samples_per_set)\n",
        "\n",
        "    # Create figure and axis\n",
        "    fig, ax = plt.subplots(1, 1, figsize=figsize)\n",
        "\n",
        "    # Create grids for both images in each pair\n",
        "    grid1 = make_grid(denormalize(val_imgs1), nrow=samples_per_set, padding=2)\n",
        "    grid2 = make_grid(denormalize(val_imgs2), nrow=samples_per_set, padding=2)\n",
        "\n",
        "    # Combine the grids vertically\n",
        "    combined_grid = torch.cat([grid1, grid2], dim=1)\n",
        "\n",
        "    # Display the combined grid\n",
        "    ax.imshow(combined_grid.permute(1, 2, 0).cpu())\n",
        "    ax.axis('off')\n",
        "    ax.set_title('Validation Pairs', fontsize=10)\n",
        "\n",
        "    # Determine dimensions for placing the labels\n",
        "    grid_width = grid1.shape[2]\n",
        "    img_width = grid_width // samples_per_set\n",
        "\n",
        "    # Add match/non-match labels for each pair\n",
        "    for i, label in enumerate(val_labels):\n",
        "        match_text = \"✓ Match\" if label == 1 else \"✗ Non-match\"\n",
        "        color = 'green' if label == 1 else 'red'\n",
        "\n",
        "        # Define a background box for the label\n",
        "        bbox_props = dict(\n",
        "            boxstyle=\"round,pad=0.3\",\n",
        "            fc=\"white\",\n",
        "            ec=color,\n",
        "            alpha=0.8\n",
        "        )\n",
        "\n",
        "        ax.text(i * img_width + img_width / 2,\n",
        "                combined_grid.shape[1] + 15,  # Position below the images\n",
        "                match_text,\n",
        "                ha='center',\n",
        "                va='top',\n",
        "                fontsize=8,\n",
        "                color=color,\n",
        "                bbox=bbox_props)\n",
        "\n",
        "    plt.suptitle(\"Verification Pairs (Top: Image 1, Bottom: Image 2)\", y=1.02)\n",
        "    plt.tight_layout()\n",
        "    plt.subplots_adjust(bottom=0.05)\n",
        "    plt.show()\n",
        "\n",
        "show_ver_dataset_samples(ver_val_loader)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y3TUocDw_JU_"
      },
      "source": [
        "# Model Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4LLX2Rki_LzA"
      },
      "outputs": [],
      "source": [
        "# TODO: Fill out the model definition below\n",
        "\n",
        "## Adopt ConvNeXtV2, reference: https://github.com/facebookresearch/ConvNeXt-V2\n",
        "\n",
        "\n",
        "from ssl import DefaultVerifyPaths\n",
        "\n",
        "\n",
        "class LayerNorm(nn.Module):\n",
        "    def __init__(self, normalized_shape, eps=1e-6, data_format=\"channels_last\"):\n",
        "        '''\n",
        "        channels_last: (batch_size, height, width, channels)\n",
        "        channels_first: (batch_size, channels, height, width)\n",
        "        '''\n",
        "        super().__init__()\n",
        "        self.weight = nn.Parameter(torch.ones(normalized_shape))\n",
        "        self.bias =  nn.Parameter(torch.zeros(normalized_shape))\n",
        "        self.eps = eps\n",
        "        self.data_format = data_format\n",
        "        if self.data_format not in [\"channels_last\", \"channels_first\"]:\n",
        "            raise NotImplementedError \n",
        "        self.normalized_shape = (normalized_shape, )\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.data_format == \"channels_last\":\n",
        "            return F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
        "        elif self.data_format == \"channels_first\":\n",
        "            u = x.mean(1, keepdim=True)\n",
        "            s = (x - u).pow(2).mean(1, keepdim=True)\n",
        "            x = (x - u) / torch.sqrt(s + self.eps)\n",
        "            x = self.weight[:, None, None] * x + self.bias[:, None, None]\n",
        "            return x\n",
        "\n",
        "# class GRN(nn.Module):\n",
        "#     ''' \n",
        "#     GRN (Global Response Normalization) layer\n",
        "#     '''\n",
        "#     def __init__(self, dim):\n",
        "#         super().__init__()\n",
        "#         self.gamma = nn.Parameter(torch.zeros(1, 1, 1, dim))\n",
        "#         self.beta = nn.Parameter(torch.zeros(1, 1, 1, dim))\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         Gx = torch.norm(x, p=2, dim=(1,2), keepdim=True)\n",
        "#         Nx = Gx / (Gx.mean(dim=-1, keepdim=True) + 1e-6)\n",
        "#         return self.gamma * (x * Nx) + self.beta + x\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, dim, drop_path=0.):\n",
        "        super().__init__()\n",
        "        self.dwconv = nn.Conv2d(dim, dim, kernel_size=7, padding=3, groups=dim)\n",
        "        self.norm = LayerNorm(dim)\n",
        "        self.pwconv1 = nn.Linear(dim, 4*dim)\n",
        "        self.activation = nn.GELU()\n",
        "        # self.grn = GRN(4*dim)\n",
        "        self.pwconv2 = nn.Linear(4*dim, dim)\n",
        "        self.drop_path = drop_path\n",
        "\n",
        "    def forward(self, x):\n",
        "        input = x\n",
        "        x = self.dwconv(x)\n",
        "        x = x.permute(0, 2, 3, 1) # (N, C, H, W) -> (N, H, W, C)\n",
        "        x = self.norm(x)\n",
        "        x = self.pwconv1(x)\n",
        "        x = self.activation(x)\n",
        "        # x = self.grn(x)\n",
        "        x = self.pwconv2(x)\n",
        "        x = x.permute(0, 3, 1, 2) # (N, H, W, C) -> (N, C, H, W)\n",
        "\n",
        "        x = input + torchvision.ops.stochastic_depth(x, p=self.drop_path, mode=\"row\")\n",
        "        return x\n",
        "\n",
        "class Network(nn.Module):\n",
        "\n",
        "    def __init__(\n",
        "        self, in_channels=3, num_classes=8631, depths=[3,3,9,3], dims=[96,192,384,768],\n",
        "        drop_path_rate=0., head_init_scale=1.\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.depths = depths\n",
        "        self.downsample_layers = nn.ModuleList()\n",
        "        stem = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, dims[0], kernel_size=4, stride=4),\n",
        "            LayerNorm(dims[0], data_format=\"channels_first\")\n",
        "        )\n",
        "        self.downsample_layers.append(stem)\n",
        "        for i in range(3):\n",
        "            downsample_layer = nn.Sequential(\n",
        "                    LayerNorm(dims[i], data_format=\"channels_first\"),\n",
        "                    nn.Conv2d(dims[i], dims[i+1], kernel_size=2, stride=2),\n",
        "            )\n",
        "            self.downsample_layers.append(downsample_layer)\n",
        "\n",
        "        self.stages = nn.ModuleList() # 4 feature resolution stages, each consisting of multiple residual blocks\n",
        "        dp_rates=[x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))] \n",
        "        cur = 0\n",
        "        for i in range(4):\n",
        "            stage = nn.Sequential(\n",
        "                *[Block(dim=dims[i], drop_path=dp_rates[cur + j]) for j in range(depths[i])]\n",
        "            )\n",
        "            self.stages.append(stage)\n",
        "            cur += depths[i]\n",
        "\n",
        "        self.norm = nn.LayerNorm(dims[-1]) # final norm layer\n",
        "        self.head = nn.Linear(dims[-1], num_classes)\n",
        "\n",
        "        self.apply(self._init_weights)\n",
        "        self.head.weight.data.mul_(head_init_scale)\n",
        "        self.head.bias.data.mul_(head_init_scale)\n",
        "\n",
        "    def _init_weights(self, m):\n",
        "        if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
        "            nn.init.trunc_normal_(m.weight, std=.2)\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward_features(self, x):\n",
        "        for i in range(4):\n",
        "            x = self.downsample_layers[i](x)\n",
        "            x = self.stages[i](x)\n",
        "        return self.norm(x.mean([-2, -1])) # global average pooling, (N, C, H, W) -> (N, C)\n",
        "\n",
        "    def forward(self, x):\n",
        "        feats = self.forward_features(x)\n",
        "        out = self.head(feats)\n",
        "        return {\"feats\": feats, \"out\": out}\n",
        "\n",
        "# Initialize your model\n",
        "model = Network(num_classes=config['num_classes'],depths=[2,2,6,2], drop_path_rate=0.1).to(DEVICE)\n",
        "\n",
        "summary(model, (config['batch_size'],3, 112, 112), device=DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pDP--pND_3Vy"
      },
      "outputs": [],
      "source": [
        "# Defining Loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Defining Optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(),lr=config['lr'],weight_decay=0.05) # TODO: Feel free to pick a optimizer\n",
        "\n",
        "# Defining Scheduler\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,factor=0.5, patience=5,threshold=0.01,mode='max',min_lr=1e-4) # TODO: Use a good scheduler such as ReduceLRonPlateau, StepLR, MultistepLR, CosineAnnealing, etc.\n",
        "\n",
        "# Initialising mixed-precision training. # Good news. We've already implemented FP16 (Mixed precision training) for you\n",
        "# It is useful only in the case of compatible GPUs such as T4/V100\n",
        "if DEVICE == 'mps':\n",
        "    scaler = torch.amp.GradScaler('mps', enabled=True)\n",
        "elif DEVICE == 'cuda':\n",
        "    scaler = torch.amp.GradScaler('cuda', enabled=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-d5ZDQfpw7gR"
      },
      "source": [
        "# Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Ecg0J2sw9jJ"
      },
      "outputs": [],
      "source": [
        "class AverageMeter:\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TqVw0ab0xBKT"
      },
      "outputs": [],
      "source": [
        "def accuracy(output, target, topk=(1,)):\n",
        "    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
        "    maxk = min(max(topk), output.size()[1])\n",
        "    batch_size = target.size(0)\n",
        "    _, pred = output.topk(maxk, 1, True, True)\n",
        "    pred = pred.t()\n",
        "    correct = pred.eq(target.reshape(1, -1).expand_as(pred))\n",
        "    return [correct[:min(k, maxk)].reshape(-1).float().sum(0) * 100. / batch_size for k in topk]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uNCQjz2RxD5S"
      },
      "outputs": [],
      "source": [
        "def get_ver_metrics(labels, scores, FPRs):\n",
        "    # eer and auc\n",
        "    fpr, tpr, _ = mt.roc_curve(labels, scores, pos_label=1)\n",
        "    roc_curve = interp1d(fpr, tpr)\n",
        "    EER = 100. * brentq(lambda x : 1. - x - roc_curve(x), 0., 1.)\n",
        "    AUC = 100. * mt.auc(fpr, tpr)\n",
        "\n",
        "    # get acc\n",
        "    tnr = 1. - fpr\n",
        "    pos_num = labels.count(1)\n",
        "    neg_num = labels.count(0)\n",
        "    ACC = 100. * max(tpr * pos_num + tnr * neg_num) / len(labels)\n",
        "\n",
        "    # TPR @ FPR\n",
        "    if isinstance(FPRs, list):\n",
        "        TPRs = [\n",
        "            ('TPR@FPR={}'.format(FPR), 100. * roc_curve(float(FPR)))\n",
        "            for FPR in FPRs\n",
        "        ]\n",
        "    else:\n",
        "        TPRs = []\n",
        "\n",
        "    return {\n",
        "        'ACC': ACC,\n",
        "        'EER': EER,\n",
        "        'AUC': AUC,\n",
        "        'TPRs': TPRs,\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "juUbZnP0AEUi"
      },
      "source": [
        "# Train and Validation Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IMnxvQT-AHsu"
      },
      "outputs": [],
      "source": [
        "# from HW2P2_Starter_Notebook import criterion\n",
        "\n",
        "\n",
        "def train_epoch(model, dataloader, optimizer, lr_scheduler, scaler, device, config):\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    # metric meters\n",
        "    loss_m = AverageMeter()\n",
        "    acc_m = AverageMeter()\n",
        "\n",
        "    # Progress Bar\n",
        "    batch_bar = tqdm(total=len(dataloader), dynamic_ncols=True, leave=False, position=0, desc='Train', ncols=5)\n",
        "\n",
        "    for i, (images, labels) in enumerate(dataloader):\n",
        "\n",
        "        optimizer.zero_grad() # Zero gradients\n",
        "\n",
        "        # send to cuda\n",
        "        images = images.to(device, non_blocking=True)\n",
        "        if isinstance(labels, (tuple, list)):\n",
        "            targets1, targets2, lam = labels\n",
        "            labels = (targets1.to(device), targets2.to(device), lam)\n",
        "        else:\n",
        "            labels = labels.to(device, non_blocking=True)\n",
        "\n",
        "        ## augmentation\n",
        "        # images, labels = cutmix_or_mixup(images, labels)\n",
        "\n",
        "        # forward\n",
        "        if DEVICE == 'cuda':\n",
        "            with torch.cuda.amp.autocast():  # This implements mixed precision. Thats it!\n",
        "                outputs = model(images)\n",
        "                # Use the type of output depending on the loss function you want to use\n",
        "                # loss = criterion(outputs['feats'], labels)\n",
        "                loss = criterion(outputs['out'], labels)\n",
        "        elif DEVICE == 'mps':\n",
        "            with torch.autocast(device_type=\"mps\", dtype=torch.bfloat16):\n",
        "                outputs = model(images)\n",
        "                # loss = criterion(outputs['feats'], labels)\n",
        "                loss = criterion(outputs['out'], labels)\n",
        "\n",
        "\n",
        "        scaler.scale(loss).backward() # This is a replacement for loss.backward()\n",
        "        scaler.step(optimizer) # This is a replacement for optimizer.step()\n",
        "        scaler.update()\n",
        "        # metrics\n",
        "        loss_m.update(loss.item())\n",
        "        # pred = logits.argmax(dim=1)\n",
        "\n",
        "        if 'feats' in outputs:\n",
        "            acc = accuracy(outputs['out'], labels)[0].item()\n",
        "            # acc = (pred == labels).float().mean().item()\n",
        "        else:\n",
        "            acc = 0.0\n",
        "        acc_m.update(acc)\n",
        "\n",
        "        # tqdm lets you add some details so you can monitor training as you train.\n",
        "        batch_bar.set_postfix(\n",
        "            # acc         = \"{:.04f}%\".format(100*accuracy),\n",
        "            acc         =\"{:.04f}% ({:.04f})\".format(acc, acc_m.avg),\n",
        "            loss        = \"{:.04f} ({:.04f})\".format(loss.item(), loss_m.avg),\n",
        "            lr          = \"{:.04f}\".format(float(optimizer.param_groups[0]['lr'])))\n",
        "\n",
        "        batch_bar.update() # Update tqdm bar\n",
        "\n",
        "    # You may want to call some schedulers inside the train function. What are these?\n",
        "    # if lr_scheduler is not None:\n",
        "    #     lr_scheduler.step(lr_scheduler)\n",
        "\n",
        "    batch_bar.close()\n",
        "\n",
        "    return acc_m.avg, loss_m.avg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5qkdH295wNUX"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def valid_epoch_cls(model, dataloader, device, config):\n",
        "\n",
        "    model.eval()\n",
        "    batch_bar = tqdm(total=len(dataloader), dynamic_ncols=True, position=0, leave=False, desc='Val Cls.', ncols=5)\n",
        "\n",
        "    # metric meters\n",
        "    loss_m = AverageMeter()\n",
        "    acc_m = AverageMeter()\n",
        "\n",
        "    for i, (images, labels) in enumerate(dataloader):\n",
        "\n",
        "        # Move images to device\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        # Get model outputs\n",
        "        with torch.inference_mode():\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs['out'], labels)\n",
        "\n",
        "        # metrics\n",
        "        acc = accuracy(outputs['out'], labels)[0].item()\n",
        "        loss_m.update(loss.item())\n",
        "        acc_m.update(acc)\n",
        "\n",
        "        batch_bar.set_postfix(\n",
        "            acc         = \"{:.04f}% ({:.04f})\".format(acc, acc_m.avg),\n",
        "            loss        = \"{:.04f} ({:.04f})\".format(loss.item(), loss_m.avg))\n",
        "\n",
        "        batch_bar.update()\n",
        "\n",
        "    batch_bar.close()\n",
        "    return acc_m.avg, loss_m.avg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Yan5vLDyj-3"
      },
      "outputs": [],
      "source": [
        "gc.collect() # These commands help you when you face CUDA OOM error\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0q1gRMAsyknz"
      },
      "source": [
        "# Verification Task"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SSGeDCi-wa1W"
      },
      "outputs": [],
      "source": [
        "def valid_epoch_ver(model, pair_data_loader, device, config):\n",
        "\n",
        "    model.eval()\n",
        "    scores = []\n",
        "    match_labels = []\n",
        "    batch_bar = tqdm(total=len(pair_data_loader), dynamic_ncols=True, position=0, leave=False, desc='Val Veri.')\n",
        "    for i, (images1, images2, labels) in enumerate(pair_data_loader):\n",
        "\n",
        "        # match_labels = match_labels.to(device)\n",
        "        images = torch.cat([images1, images2], dim=0).to(device)\n",
        "        # Get model outputs\n",
        "        with torch.inference_mode():\n",
        "            outputs = model(images)\n",
        "\n",
        "        feats = F.normalize(outputs['feats'], dim=1)\n",
        "        feats1, feats2 = feats.chunk(2)\n",
        "        similarity = F.cosine_similarity(feats1, feats2)\n",
        "        scores.append(similarity.cpu().numpy())\n",
        "        match_labels.append(labels.cpu().numpy())\n",
        "        batch_bar.update()\n",
        "\n",
        "    scores = np.concatenate(scores)\n",
        "    match_labels = np.concatenate(match_labels)\n",
        "\n",
        "    FPRs=['1e-4', '5e-4', '1e-3', '5e-3', '5e-2']\n",
        "    metric_dict = get_ver_metrics(match_labels.tolist(), scores.tolist(), FPRs)\n",
        "    print(metric_dict)\n",
        "\n",
        "    return metric_dict['ACC'],metric_dict['EER']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "piblCbe5yotj"
      },
      "source": [
        "# WandB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HTIkCXBQyoM0"
      },
      "outputs": [],
      "source": [
        "# wandb.login(key=\"TODO\") # API Key is in your wandb account, under settings (wandb.ai/settings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GLNNqwV4ysNP"
      },
      "outputs": [],
      "source": [
        "# # Create your wandb run\n",
        "# wandb_name = TODO\n",
        "# run = wandb.init(\n",
        "#     name = wandb_name, ## Wandb creates random run names if you skip this field\n",
        "#     reinit = True, ### Allows reinitalizing runs when you re-run this cell\n",
        "#     # run_id = ### Insert specific run id here if you want to resume a previous run\n",
        "#     # resume = \"must\" ### You need this to resume previous runs, but comment out reinit = True when using this\n",
        "#     project = \"TODO\", ### Project should be created in your wandb account\n",
        "#     config = config ### Wandb Config for your run\n",
        "# )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t0RrtpFKzH3k"
      },
      "source": [
        "# Checkpointing and Loading Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O1gbAkMtlWHk"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "checkpoint_dir = config['checkpoint_dir']\n",
        "\n",
        "# Create the directory if it doesn't exist\n",
        "os.makedirs(checkpoint_dir, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dDFmC8hpzLOq"
      },
      "outputs": [],
      "source": [
        "def save_model(model, optimizer, scheduler, metrics, epoch, path):\n",
        "    torch.save(\n",
        "        {'model_state_dict'         : model.state_dict(),\n",
        "         'optimizer_state_dict'     : optimizer.state_dict(),\n",
        "         'scheduler_state_dict'     : scheduler.state_dict(),\n",
        "         'metric'                   : metrics,\n",
        "         'epoch'                    : epoch},\n",
        "         path)\n",
        "\n",
        "\n",
        "def load_model(model, optimizer=None, scheduler=None, path='./checkpoint.pth'):\n",
        "    checkpoint = torch.load(path, weights_only=False)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    if optimizer is not None:\n",
        "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    else:\n",
        "        optimizer = None\n",
        "    if scheduler is not None:\n",
        "        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
        "    else:\n",
        "        scheduler = None\n",
        "    epoch = checkpoint['epoch']\n",
        "    metrics = checkpoint['metric']\n",
        "    return model, optimizer, scheduler, epoch, metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wpFT7iriy5bi"
      },
      "source": [
        "# Experiments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "59FcCeJfy3Zm"
      },
      "outputs": [],
      "source": [
        "# model, _, _, _, _ = load_model(model, path='/TODO.pth')\n",
        "\n",
        "e = 0\n",
        "best_valid_cls_acc = 0.0\n",
        "eval_cls = True\n",
        "best_valid_ret_acc = 0.0\n",
        "# valid_cls_acc = 0.0\n",
        "for epoch in range(e, config['epochs']):\n",
        "    # epoch\n",
        "    print(\"\\nEpoch {}/{}\".format(epoch+1, config['epochs']))\n",
        "\n",
        "    # train\n",
        "    train_cls_acc, train_loss = train_epoch(model, cls_train_loader, optimizer, scheduler, scaler, DEVICE, config)\n",
        "    curr_lr = float(optimizer.param_groups[0]['lr'])\n",
        "    print(\"\\nEpoch {}/{}: \\nTrain Cls. Acc {:.04f}%\\t Train Cls. Loss {:.04f}\\t Learning Rate {:.04f}\".format(epoch + 1, config['epochs'], train_cls_acc, train_loss, curr_lr))\n",
        "    metrics = {\n",
        "        'train_cls_acc': train_cls_acc,\n",
        "        'train_loss': train_loss,\n",
        "        'lr': curr_lr\n",
        "    }\n",
        "    # classification validation\n",
        "    if eval_cls:\n",
        "        valid_cls_acc, valid_loss = valid_epoch_cls(model, cls_val_loader, DEVICE, config)\n",
        "        print(\"Val Cls. Acc {:.04f}%\\t Val Cls. Loss {:.04f}\".format(valid_cls_acc, valid_loss))\n",
        "        metrics.update({\n",
        "            'valid_cls_acc': valid_cls_acc,\n",
        "            'valid_loss': valid_loss,\n",
        "        })\n",
        "\n",
        "    # retrieval validation\n",
        "    valid_ret_acc, valid_ret_eer = valid_epoch_ver(model, ver_val_loader, DEVICE, config)\n",
        "    print(\"Val Ret. Acc {:.04f}%\".format(valid_ret_acc))\n",
        "    metrics.update({\n",
        "        'valid_ret_acc': valid_ret_acc,\n",
        "        'valid_ret_eer': valid_ret_eer\n",
        "    })\n",
        "\n",
        "    scheduler.step(valid_cls_acc)\n",
        "\n",
        "    # save model\n",
        "    save_model(model, optimizer, scheduler, metrics, epoch, os.path.join(config['checkpoint_dir'], f'{wandb_name}_last.pth'))\n",
        "    print(\"Saved epoch model\")\n",
        "\n",
        "    # save best model\n",
        "    if eval_cls:\n",
        "        if valid_cls_acc >= best_valid_cls_acc:\n",
        "            best_valid_cls_acc = valid_cls_acc\n",
        "            save_model(model, optimizer, scheduler, metrics, epoch, os.path.join(config['checkpoint_dir'], f'{wandb_name}_best_cls.pth'))\n",
        "            # wandb.save(f'{wandb_name}_best_cls.pth')\n",
        "            print(\"Saved best classification model\")\n",
        "\n",
        "    if valid_ret_acc >= best_valid_ret_acc:\n",
        "        best_valid_ret_acc = valid_ret_acc\n",
        "        save_model(model, optimizer, scheduler, metrics, epoch, os.path.join(config['checkpoint_dir'], f'{wandb_name}_best_ret.pth'))\n",
        "        # wandb.save(f'{wandb_name}_best_cls.pth')\n",
        "        print(\"Saved best retrieval model\")\n",
        "\n",
        "\n",
        "    if curr_lr < 1e-4:\n",
        "        print(\"Early stop by LR.\")\n",
        "        break\n",
        "\n",
        "        "
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "P5gfdJzx3kG8",
        "EutBOttx3kG8",
        "62j2RAEA3kG8",
        "HavF1IlO3kG9",
        "d5l6iDrZ3kG9",
        "Rsnoo8SU3kG9",
        "da-B39u83kG9"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
